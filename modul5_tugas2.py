# -*- coding: utf-8 -*-
"""Modul5_tugas2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GAS8cTxuLrBqnqO3q4HM3PK8pLAwnyWq
"""

!pip install transformers

!pip install wordcloud

# Import Library
import pandas as pd
import tensorflow as tf
from transformers import BertTokenizer, TFBertForSequenceClassification
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import numpy as np

import nltk
nltk.download('wordnet')

df = pd.read_csv("cleaned_reviews.csv")
df.head()

df.info()

df = df.drop(['cleaned_review_length','review_score'],axis=1)
df.columns = ["sentiment","review"]
df.head()

df.isnull().sum()

df.dropna(inplace=True)

df.isnull().sum()

labeling = {
    'neutral' :2,
    'positive':1,
    'negative':0
}

df['sentiment'] = df['sentiment'].apply(lambda x : labeling[x])
# Output first ten rows
df.head(10)

# The distribution of sentiments
df.groupby('sentiment').count().plot(kind='bar')

# Calculate review lengths
review_len = pd.Series([len(review.split()) for review in df['review']])

# The distribution of review text lengths
review_len.plot(kind='box')

import seaborn as sns
import matplotlib.pyplot as plt

sns.set_theme(
    context='notebook',
    style='darkgrid',
    palette='deep',
    font='sans-serif',
    font_scale=1,
    color_codes=True,
    rc=None,
)

plt.figure(figsize = (10,12))
sns.histplot(review_len)

fig = plt.figure(figsize=(14,7))
df['length'] = df.review.str.split().apply(len)
ax1 = fig.add_subplot(122)
sns.histplot(df[df['sentiment']==1]['length'], ax=ax1,color='green')
describe = df.length[df.sentiment==1].describe().to_frame().round(2)

ax2 = fig.add_subplot(121)
ax2.axis('off')
font_size = 14
bbox = [0, 0, 1, 1]
table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)
table.set_fontsize(font_size)
fig.suptitle('Distribution of text length for positive sentiment reviews.', fontsize=16)

plt.show()

fig = plt.figure(figsize=(14,7))
df['length'] = df.review.str.split().apply(len)
ax1 = fig.add_subplot(122)
sns.histplot(df[df['sentiment']==1]['length'], ax=ax1,color='red')
describe = df.length[df.sentiment==0].describe().to_frame().round(2)

ax2 = fig.add_subplot(121)
ax2.axis('off')
font_size = 14
bbox = [0, 0, 1, 1]
table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)
table.set_fontsize(font_size)
fig.suptitle('Distribution of text length for negative sentiment reviews.', fontsize=16)

plt.show()

fig = plt.figure(figsize=(14,7))
df['length'] = df.review.str.split().apply(len)
ax1 = fig.add_subplot(122)
sns.histplot(df[df['sentiment']==1]['length'], ax=ax1,color='grey')
describe = df.length[df.sentiment==2].describe().to_frame().round(2)

ax2 = fig.add_subplot(121)
ax2.axis('off')
font_size = 14
bbox = [0, 0, 1, 1]
table = ax2.table(cellText = describe.values, rowLabels = describe.index, bbox=bbox, colLabels=describe.columns)
table.set_fontsize(font_size)
fig.suptitle('Distribution of text length for neutral sentiment reviews.', fontsize=16)

plt.show()

#negative text
from wordcloud import WordCloud
plt.figure(figsize = (20,20)) # Negative Review Text
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.sentiment == 0].review))
plt.imshow(wc , interpolation = 'bilinear')

#positive text
from wordcloud import WordCloud
plt.figure(figsize = (20,20)) # Positive Review Text
wc = WordCloud(max_words = 2000 , width = 1600 , height = 800).generate(" ".join(df[df.sentiment == 1].review))
plt.imshow(wc , interpolation = 'bilinear')

# Mengunduh dataset kata-kata stop dalam bahasa Inggris dari NLTK.
import nltk
from nltk.corpus import wordnet
nltk.download('stopwords')

# Mengimpor modul stopwords dari nltk.corpus untuk mengakses kata-kata stop.
from nltk.corpus import stopwords

# Membuat sebuah list kosong
stop_words = []

# Melakukan iterasi melalui setiap kata dalam daftar kata-kata stop dalam bahasa Inggris.
for w in stopwords.words('english'):
    # Menambahkan setiap kata stop ke dalam list.
    stop_words.append(w)

# Mencetak list yang berisi kata-kata stop dalam bahasa Inggris.
print(stop_words)

def get_synonyms(word):
    # Membuat sebuah set kosong untuk menyimpan sinonim.
    synonyms = set()

    # Melakukan iterasi melalui setiap synset (kumpulan kata dengan makna yang mirip)
    # dalam WordNet yang terkait dengan kata yang diberikan.
    for syn in wordnet.synsets(word):
        # Melakukan iterasi melalui setiap lemma (kata turunan) dalam synset.
        for l in syn.lemmas():
            # Mendapatkan kata sinonim, mengganti karakter khusus seperti _ dan - dengan spasi, mengubah huruf menjadi huruf kecil
            synonym = l.name().replace("_", " ").replace("-", " ").lower()
            synonym = "".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm']) # membersihkan karakter yang bukan huruf.

            # Menambahkan sinonim ke dalam set sinonim.
            synonyms.add(synonym)

    # Menghapus kata awal (kata yang diberikan) dari set sinonim, jika ada.
    if word in synonyms:
        synonyms.remove(word)

    # Mengembalikan list dari sinonim-sinonim yang ditemukan.
    return list(synonyms)

def synonym_replacement(words, n):
    # Memisahkan kata-kata dalam kalimat menjadi sebuah list dengan .split().
    words = words.split()

    # Membuat salinan list
    new_words = words.copy()
    # Membuat daftar kata-kata unik yang bukan merupakan kata-kata stop words.
    random_word_list = list(set([word for word in words if word not in stop_words]))
    # Mengacak urutan kata-kata unik.
    random.shuffle(random_word_list)

    # Inisialisasi variabel untuk menghitung berapa banyak kata yang sudah digantikan.
    num_replaced = 0

    # Melakukan iterasi melalui daftar kata-kata unik yang telah diacak.
    for random_word in random_word_list:
        # Mendapatkan sinonim-sinonim dari kata acak dengan fungsi get_synonyms() yang kita buat sebelumnya.
        synonyms = get_synonyms(random_word)

        # Jika ada setidaknya satu sinonim yang ditemukan.
        if len(synonyms) >= 1:
            # Memilih salah satu sinonim secara acak.
            synonym = random.choice(list(synonyms))

            # Mengganti kata acak dengan sinonimnya dalam new_words.
            new_words = [synonym if word == random_word else word for word in new_words]

            # Menambah jumlah kata yang sudah digantikan.
            num_replaced += 1

        # Jika jumlah kata yang sudah digantikan mencapai n, maka berhenti.
        if num_replaced >= n:
            break

    # Menggabungkan kata-kata dalam new_words menjadi sebuah kalimat.
    sentence = ' '.join(new_words)

    # Mengembalikan nilai dari fungsi menjadi sebuah kalimat dari kata-kata.
    return sentence

def random_deletion(words, p):

    words = words.split()

    # Jika kalimat hanya terdiri dari satu kata, maka kalimat tersebut dikembalikan tanpa perubahan.
    if len(words) == 1:
        return words

    # Membuat list baru untuk kata-kata yang akan dipertahankan.
    new_words = []

    # Melakukan iterasi melalui kata-kata dalam kalimat.
    for word in words:
        # Menghasilkan angka acak antara 0 dan 1.
        r = random.uniform(0, 1)

        # Jika nilai acak r lebih besar dari p (probabilitas), maka kata tersebut akan dipertahankan dengan menambahkannya ke dalam list new_words.
        if r > p:
            new_words.append(word)

    # Jika tidak ada kata yang dipertahankan (semua kata dihapus), maka satu kata acak dipilih untuk tetap ada dalam kalimat.
    if len(new_words) == 0:
        rand_int = random.randint(0, len(words)-1)
        return [words[rand_int]]

    # Menggabungkan kata-kata yang dipertahankan dalam list menjadi sebuah kalimat.
    sentence = ' '.join(new_words)

    return sentence

def swap_word(new_words):
    # Menghasilkan dua indeks acak dalam range dari 0 hingga panjang new_words - 1.
    random_idx_1 = random.randint(0, len(new_words)-1)
    random_idx_2 = random_idx_1
    counter = 0

    # Melakukan loop sampai indeks kedua tidak sama dengan indeks pertama.
    while random_idx_2 == random_idx_1:
        random_idx_2 = random.randint(0, len(new_words)-1)
        counter += 1

        # Jika counter melebihi 3, maka kembalikan new_words tanpa perubahan.
        if counter > 3:
            return new_words

    # Menukar kata di indeks pertama dengan kata di indeks kedua dalam new_words.
    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1]

    # Mengembalikan new_words setelah kata-kata ditukar.
    return new_words

def random_swap(words, n):

    words = words.split()
    # Membuat salinan list kata-kata yang akan dimodifikasi.
    new_words = words.copy()

    # Melakukan pertukaran kata-kata sebanyak n kali.
    for _ in range(n):
        new_words = swap_word(new_words)  # Memanggil fungsi swap_word() yang sudah dibuat untuk menukar kata-kata.

    # Menggabungkan kata-kata yang sudah ditukar kembali menjadi sebuah kalimat.
    sentence = ' '.join(new_words)

    # Mengembalikan kalimat setelah semua pertukaran kata-kata selesai.
    return sentence

# Menampilkan data pada dataframe indeks 1
words = df['review'][1]
print(words)

import random
# Penerapan Synonym Replacement dengan fungsi synonym_replacement(words,n)
for n in range(5):
    print(f" Example of Synonym Replacement: {synonym_replacement(words,n)}")

# Penerapan Random Deletion dengan fungsi random_deletion(words,n)
print("Original:", words)
print('\nRandom Deletion:')
print(random_deletion(words,0.2))
print(random_deletion(words,0.3))
print(random_deletion(words,0.4))

# Penerapan Easy Data Augmentation pada dataset tipe teks menggunakan Synonym Replacement, Random Swap dan Random Deletion

augmented_text = []
for n in range(1000):
    word = df['review'][n]

    synonym_replaced = synonym_replacement(word, 3)
    augmented_text.append(synonym_replaced)

    random_swapped = random_swap(word, 3)
    augmented_text.append(random_swapped)

    random_deleted = random_deletion(word, 0.2)
    augmented_text.append(random_deleted)

# Mengubah list di atas menjadi Dataframe
augmented_df = pd.DataFrame(augmented_text, columns=['Augmented_Text'])

# Mengubah list di atas menjadi Dataframe
augmented_df = pd.DataFrame(augmented_text, columns=['Augmented_Text'])

augmented_df

df2 = pd.concat([df, augmented_df], axis=1)
df2

df2.isna()

df2.isna().sum()

# Drop rows with NaN values in the 'sentiment' column
df2_cleaned = df2.dropna()

# Print the cleaned DataFrame
print(df2_cleaned)

# Assume df_cleaned is your processed DataFrame
df2_cleaned.to_csv('D:\Kuliah\Semester 7\Modul Prak Pembelajaran Mesin\Modul 5 - Transfer Learning\Dataset\processed_data.csv', index=False)

df2['Augmented_Text'] = df2['Augmented_Text'].astype(str)

df2.info()

df2.replace({0.0: 0, 1.0: 1, 2.0: 2}, inplace=True)

# Contoh: Mengganti nilai string "NaN" dengan NaN
df2.replace("nan", pd.NA, inplace=True)

# Drop rows with NaN values in the 'sentiment' column
df2_cleaned = df2.dropna()

# Print the cleaned DataFrame
print(df2_cleaned)

unique_sentiments = df2_cleaned['sentiment'].unique()
unique_sentiments

df2_cleaned.isna().sum()

texts = df2_cleaned['Augmented_Text'].tolist()
labels = df2_cleaned['sentiment'].tolist()

# Gunakan train_test_split untuk membagi dataset
train_texts, test_texts, train_labels, test_labels = train_test_split(texts, labels, test_size=0.2, random_state=111)

# Tampilkan ukuran setiap bagian
print("Jumlah data pelatihan:", len(train_texts))
print("Jumlah data pengujian:", len(test_texts))

# Menggunakan tokenizer BERT
tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=128)
test_encodings = tokenizer(test_texts, truncation=True, padding=True, max_length=128)

# Membuat dataset TensorFlow
train_dataset = tf.data.Dataset.from_tensor_slices((
    dict(train_encodings),
    train_labels
))
test_dataset = tf.data.Dataset.from_tensor_slices((
    dict(test_encodings),
    test_labels
))

# Memuat model BERT untuk klasifikasi teks (contoh: sentimen positif/negatif)
model = TFBertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)

# Proses pelatihan
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)
model.compile(optimizer=optimizer, loss=model.hf_compute_loss, metrics=['accuracy'])
model.fit(train_dataset.shuffle(100).batch(16), epochs=5, batch_size=16)

# Evaluasi Model
loss, accuracy = model.evaluate(test_dataset.batch(16))
print("Akurasi:", accuracy)
print("Loss:", loss)

# Prediksi Model
predictions = model.predict(test_dataset.batch(16))
predicted_classes = tf.argmax(predictions.logits, axis=1)

# Menampilkan laporan klasifikasi (0 = Negative, 1 = Positive, 2 = Neutral)
target_names = ['Negative', 'Positive', 'Neutral']
print(classification_report(test_labels, predicted_classes, target_names=target_names))

from sklearn.metrics import classification_report
import numpy as np


# Prediksi Model
predictions = model.predict(test_dataset.batch(16))
predicted_classes = np.argmax(predictions.logits, axis=1)

# Konversi label TensorFlow ke array NumPy
test_labels_numpy = np.array(test_labels)

# Menampilkan laporan klasifikasi (0 = Negative, 1 = Positive, 2 = Neutral)
target_names = ['Negative', 'Positive', 'Neutral']
print(classification_report(test_labels_numpy, predicted_classes, target_names=target_names))

# Sample data (replace with your actual data)
data = df['review'].tolist()
labels = df['sentiment'].tolist()

# Split the data
train_data, test_data, train_labels, test_labels = train_test_split(data, labels, test_size=0.2, random_state=111)

# Create a RandomForestClassifier
rf_model = RandomForestClassifier(n_estimators=100, random_state=111)

# Fit the model on the training data
rf_model.fit(train_data, train_labels)

# Make predictions on the test data
predictions = rf_model.predict(test_data)

# Display the classification report
target_names = ['Class 0', 'Class 1', 'Class 2']  # Replace with your actual class names
print(classification_report(test_labels, predictions, target_names=target_names))

